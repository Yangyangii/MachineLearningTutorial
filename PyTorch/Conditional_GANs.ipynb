{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Conditional GANs\n",
    "Reference: https://arxiv.org/pdf/1411.1784.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_cuda(x):\n",
    "    x = Variable(x)\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_onehot(x, num_classes=10):\n",
    "    assert isinstance(x, int) or isinstance(x.data, (torch.cuda.LongTensor, torch.LongTensor))    \n",
    "    if isinstance(x, int):\n",
    "        c = to_cuda(torch.zeros(1, 10))\n",
    "        c.data[0][x] = 1\n",
    "    else:\n",
    "        c = to_cuda(torch.FloatTensor(x.size(0), num_classes))\n",
    "        c.zero_()\n",
    "        c.scatter_(1, x, 1) # dim, index, src value\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Discriminator w/ MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=784, label_size=10, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size+label_size, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(200, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):        \n",
    "        x, y = x.view(x.size(0), -1), y.view(y.size(0), -1)\n",
    "        v = torch.cat((x, y), 1) # v: [input, label] concatenated vector\n",
    "        y_ = self.layer1(v)\n",
    "        y_ = self.layer2(y_)\n",
    "        y_ = self.layer3(y_)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Generator w/ MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=100, label_size=10, num_classes=784):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size+label_size, 200),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(200, num_classes),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x, y = x.view(x.size(0), -1), y.view(y.size(0), -1)\n",
    "        v = torch.cat((x, y), 1) # v: [input, label] concatenated vector\n",
    "        y_ = self.layer(v)\n",
    "        y_ = y_.view(x.size(0), 1, 28, 28)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = Discriminator().cuda()\n",
    "G = Generator().cuda()\n",
    "D.load_state_dict('D_c.pkl')\n",
    "G.load_state_dict('G_c.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                std=(0.5, 0.5, 0.5))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST(root='../data/', train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "condition_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=mnist, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "D_opt = torch.optim.Adam(D.parameters())\n",
    "G_opt = torch.optim.Adam(G.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epoch = 200 # need more than 500 epochs for training generator\n",
    "step = 0\n",
    "n_critic = 5 # for training more k steps about Discriminator\n",
    "n_noise = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyangii/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Step: 1000, D Loss: 1.0565564632415771, G Loss: 1.0446336269378662\n",
      "Epoch: 2/200, Step: 2000, D Loss: 1.19319486618042, G Loss: 1.1084250211715698\n",
      "Epoch: 3/200, Step: 3000, D Loss: 1.1244301795959473, G Loss: 1.14289391040802\n",
      "Epoch: 4/200, Step: 4000, D Loss: 1.1763558387756348, G Loss: 1.0058834552764893\n",
      "Epoch: 5/200, Step: 5000, D Loss: 1.2633025646209717, G Loss: 1.0351911783218384\n",
      "Epoch: 6/200, Step: 6000, D Loss: 1.2585111856460571, G Loss: 1.1054434776306152\n",
      "Epoch: 7/200, Step: 7000, D Loss: 1.2069371938705444, G Loss: 0.9803129434585571\n",
      "Epoch: 8/200, Step: 8000, D Loss: 1.276810646057129, G Loss: 0.8564074039459229\n",
      "Epoch: 9/200, Step: 9000, D Loss: 1.1529862880706787, G Loss: 1.2688381671905518\n",
      "Epoch: 10/200, Step: 10000, D Loss: 1.1455817222595215, G Loss: 0.9047998785972595\n",
      "Epoch: 11/200, Step: 11000, D Loss: 1.1705278158187866, G Loss: 1.0975396633148193\n",
      "Epoch: 12/200, Step: 12000, D Loss: 1.1406861543655396, G Loss: 0.8779134750366211\n",
      "Epoch: 13/200, Step: 13000, D Loss: 1.2513489723205566, G Loss: 1.0898805856704712\n",
      "Epoch: 14/200, Step: 14000, D Loss: 1.1546627283096313, G Loss: 0.9679872989654541\n",
      "Epoch: 16/200, Step: 15000, D Loss: 1.265885591506958, G Loss: 0.9487227201461792\n",
      "Epoch: 17/200, Step: 16000, D Loss: 1.1905803680419922, G Loss: 1.1251782178878784\n",
      "Epoch: 18/200, Step: 17000, D Loss: 1.2219535112380981, G Loss: 0.8439731597900391\n",
      "Epoch: 19/200, Step: 18000, D Loss: 1.110572099685669, G Loss: 0.9290933609008789\n",
      "Epoch: 20/200, Step: 19000, D Loss: 1.282396912574768, G Loss: 1.0876721143722534\n",
      "Epoch: 21/200, Step: 20000, D Loss: 1.1962296962738037, G Loss: 0.8943583369255066\n",
      "Epoch: 22/200, Step: 21000, D Loss: 1.088798999786377, G Loss: 1.0283997058868408\n",
      "Epoch: 23/200, Step: 22000, D Loss: 1.16806960105896, G Loss: 0.9272487163543701\n",
      "Epoch: 24/200, Step: 23000, D Loss: 1.1178942918777466, G Loss: 1.0733553171157837\n",
      "Epoch: 25/200, Step: 24000, D Loss: 1.205965280532837, G Loss: 0.9438983798027039\n",
      "Epoch: 26/200, Step: 25000, D Loss: 1.0734140872955322, G Loss: 1.0498088598251343\n",
      "Epoch: 27/200, Step: 26000, D Loss: 1.146719217300415, G Loss: 0.9132895469665527\n",
      "Epoch: 28/200, Step: 27000, D Loss: 1.1082162857055664, G Loss: 1.0636012554168701\n",
      "Epoch: 29/200, Step: 28000, D Loss: 1.2817480564117432, G Loss: 0.9803779125213623\n",
      "Epoch: 30/200, Step: 29000, D Loss: 1.1663036346435547, G Loss: 0.9071440696716309\n",
      "Epoch: 32/200, Step: 30000, D Loss: 1.2211179733276367, G Loss: 0.8799716234207153\n",
      "Epoch: 33/200, Step: 31000, D Loss: 1.1141514778137207, G Loss: 1.2148513793945312\n",
      "Epoch: 34/200, Step: 32000, D Loss: 1.2322677373886108, G Loss: 0.9768485426902771\n",
      "Epoch: 35/200, Step: 33000, D Loss: 1.1878776550292969, G Loss: 0.9473999738693237\n",
      "Epoch: 36/200, Step: 34000, D Loss: 1.099670171737671, G Loss: 1.000373363494873\n",
      "Epoch: 37/200, Step: 35000, D Loss: 1.2489221096038818, G Loss: 1.0123194456100464\n",
      "Epoch: 38/200, Step: 36000, D Loss: 1.1592870950698853, G Loss: 1.4178578853607178\n",
      "Epoch: 39/200, Step: 37000, D Loss: 1.153398036956787, G Loss: 0.8338032364845276\n",
      "Epoch: 40/200, Step: 38000, D Loss: 1.1472364664077759, G Loss: 1.2750353813171387\n",
      "Epoch: 41/200, Step: 39000, D Loss: 1.1600000858306885, G Loss: 0.9025933742523193\n",
      "Epoch: 42/200, Step: 40000, D Loss: 1.1974786520004272, G Loss: 0.919741153717041\n",
      "Epoch: 43/200, Step: 41000, D Loss: 1.230983018875122, G Loss: 0.9596996307373047\n",
      "Epoch: 44/200, Step: 42000, D Loss: 1.304158091545105, G Loss: 0.8720695972442627\n",
      "Epoch: 45/200, Step: 43000, D Loss: 1.1083123683929443, G Loss: 0.854911744594574\n",
      "Epoch: 46/200, Step: 44000, D Loss: 1.082154631614685, G Loss: 0.8382217884063721\n",
      "Epoch: 48/200, Step: 45000, D Loss: 1.1276969909667969, G Loss: 1.2226359844207764\n",
      "Epoch: 49/200, Step: 46000, D Loss: 1.2056137323379517, G Loss: 1.074927568435669\n",
      "Epoch: 50/200, Step: 47000, D Loss: 1.0966796875, G Loss: 0.8638730645179749\n",
      "Epoch: 51/200, Step: 48000, D Loss: 1.146012783050537, G Loss: 0.8330654501914978\n",
      "Epoch: 52/200, Step: 49000, D Loss: 1.106101632118225, G Loss: 1.0650970935821533\n",
      "Epoch: 53/200, Step: 50000, D Loss: 1.3344666957855225, G Loss: 0.8841290473937988\n",
      "Epoch: 54/200, Step: 51000, D Loss: 1.075491189956665, G Loss: 1.0720493793487549\n",
      "Epoch: 55/200, Step: 52000, D Loss: 1.2420897483825684, G Loss: 0.9262038469314575\n",
      "Epoch: 56/200, Step: 53000, D Loss: 1.2491447925567627, G Loss: 0.9758824110031128\n",
      "Epoch: 57/200, Step: 54000, D Loss: 1.1456325054168701, G Loss: 0.9346328377723694\n",
      "Epoch: 58/200, Step: 55000, D Loss: 1.1827744245529175, G Loss: 1.0644336938858032\n",
      "Epoch: 59/200, Step: 56000, D Loss: 0.9112427234649658, G Loss: 1.558779001235962\n",
      "Epoch: 60/200, Step: 57000, D Loss: 1.2426015138626099, G Loss: 0.924956202507019\n",
      "Epoch: 61/200, Step: 58000, D Loss: 1.1365265846252441, G Loss: 0.8205435276031494\n",
      "Epoch: 62/200, Step: 59000, D Loss: 1.220055341720581, G Loss: 1.0278922319412231\n",
      "Epoch: 64/200, Step: 60000, D Loss: 1.1696889400482178, G Loss: 0.887438952922821\n",
      "Epoch: 65/200, Step: 61000, D Loss: 1.2594373226165771, G Loss: 0.8833436965942383\n",
      "Epoch: 66/200, Step: 62000, D Loss: 1.1478979587554932, G Loss: 0.8321018218994141\n",
      "Epoch: 67/200, Step: 63000, D Loss: 0.9577665328979492, G Loss: 1.1188209056854248\n",
      "Epoch: 68/200, Step: 64000, D Loss: 1.2404162883758545, G Loss: 1.015075922012329\n",
      "Epoch: 69/200, Step: 65000, D Loss: 1.1131212711334229, G Loss: 0.9771844148635864\n",
      "Epoch: 70/200, Step: 66000, D Loss: 1.1481235027313232, G Loss: 0.869774341583252\n",
      "Epoch: 71/200, Step: 67000, D Loss: 1.2196630239486694, G Loss: 0.8219259977340698\n",
      "Epoch: 72/200, Step: 68000, D Loss: 1.2583674192428589, G Loss: 0.8857836127281189\n",
      "Epoch: 73/200, Step: 69000, D Loss: 1.2861384153366089, G Loss: 0.8206833600997925\n",
      "Epoch: 74/200, Step: 70000, D Loss: 1.2289483547210693, G Loss: 1.0290346145629883\n",
      "Epoch: 75/200, Step: 71000, D Loss: 1.2467825412750244, G Loss: 0.8436188697814941\n",
      "Epoch: 76/200, Step: 72000, D Loss: 1.3024102449417114, G Loss: 0.8879547119140625\n",
      "Epoch: 77/200, Step: 73000, D Loss: 1.1655304431915283, G Loss: 0.9281982779502869\n",
      "Epoch: 78/200, Step: 74000, D Loss: 1.188279151916504, G Loss: 0.9923003911972046\n",
      "Epoch: 80/200, Step: 75000, D Loss: 1.2217304706573486, G Loss: 0.7951579093933105\n",
      "Epoch: 81/200, Step: 76000, D Loss: 1.2738139629364014, G Loss: 0.9352043867111206\n",
      "Epoch: 82/200, Step: 77000, D Loss: 1.2140896320343018, G Loss: 1.0559980869293213\n",
      "Epoch: 83/200, Step: 78000, D Loss: 1.2604347467422485, G Loss: 0.9272066354751587\n",
      "Epoch: 84/200, Step: 79000, D Loss: 1.076514482498169, G Loss: 1.3430898189544678\n",
      "Epoch: 85/200, Step: 80000, D Loss: 1.2233009338378906, G Loss: 1.0443544387817383\n",
      "Epoch: 86/200, Step: 81000, D Loss: 1.2627192735671997, G Loss: 0.9016692638397217\n",
      "Epoch: 87/200, Step: 82000, D Loss: 1.201853632926941, G Loss: 1.014427900314331\n",
      "Epoch: 88/200, Step: 83000, D Loss: 1.1724035739898682, G Loss: 0.8757964372634888\n",
      "Epoch: 89/200, Step: 84000, D Loss: 1.3991641998291016, G Loss: 0.9686983823776245\n",
      "Epoch: 90/200, Step: 85000, D Loss: 1.2253190279006958, G Loss: 1.057689905166626\n",
      "Epoch: 91/200, Step: 86000, D Loss: 1.1524590253829956, G Loss: 0.8801630139350891\n",
      "Epoch: 92/200, Step: 87000, D Loss: 1.2404884099960327, G Loss: 0.8715023994445801\n",
      "Epoch: 93/200, Step: 88000, D Loss: 1.1633970737457275, G Loss: 0.9406765103340149\n",
      "Epoch: 94/200, Step: 89000, D Loss: 1.2135523557662964, G Loss: 1.0079668760299683\n",
      "Epoch: 96/200, Step: 90000, D Loss: 1.127681016921997, G Loss: 0.8956422805786133\n",
      "Epoch: 97/200, Step: 91000, D Loss: 1.266331672668457, G Loss: 0.8012330532073975\n",
      "Epoch: 98/200, Step: 92000, D Loss: 1.127396583557129, G Loss: 1.1796472072601318\n",
      "Epoch: 99/200, Step: 93000, D Loss: 1.2991149425506592, G Loss: 0.8927134275436401\n",
      "Epoch: 100/200, Step: 94000, D Loss: 1.1299326419830322, G Loss: 0.976744532585144\n",
      "Epoch: 101/200, Step: 95000, D Loss: 1.2752399444580078, G Loss: 1.0472911596298218\n",
      "Epoch: 102/200, Step: 96000, D Loss: 1.279223918914795, G Loss: 1.1737961769104004\n",
      "Epoch: 103/200, Step: 97000, D Loss: 1.109688401222229, G Loss: 0.8530181646347046\n",
      "Epoch: 104/200, Step: 98000, D Loss: 1.166578769683838, G Loss: 0.8945108652114868\n",
      "Epoch: 105/200, Step: 99000, D Loss: 1.0421591997146606, G Loss: 0.9943660497665405\n",
      "Epoch: 106/200, Step: 100000, D Loss: 1.221508502960205, G Loss: 1.0534470081329346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107/200, Step: 101000, D Loss: 1.164905309677124, G Loss: 0.9452950358390808\n",
      "Epoch: 108/200, Step: 102000, D Loss: 1.145942211151123, G Loss: 1.1962862014770508\n",
      "Epoch: 109/200, Step: 103000, D Loss: 1.3542085886001587, G Loss: 0.8296689391136169\n",
      "Epoch: 110/200, Step: 104000, D Loss: 1.1116877794265747, G Loss: 0.8952528238296509\n",
      "Epoch: 112/200, Step: 105000, D Loss: 1.2182989120483398, G Loss: 0.9432944655418396\n",
      "Epoch: 113/200, Step: 106000, D Loss: 1.1311761140823364, G Loss: 0.8646923899650574\n",
      "Epoch: 114/200, Step: 107000, D Loss: 1.1970000267028809, G Loss: 0.8631386756896973\n",
      "Epoch: 115/200, Step: 108000, D Loss: 1.3550679683685303, G Loss: 1.1321603059768677\n",
      "Epoch: 116/200, Step: 109000, D Loss: 1.1937726736068726, G Loss: 1.06623375415802\n",
      "Epoch: 117/200, Step: 110000, D Loss: 1.164794921875, G Loss: 1.0883926153182983\n",
      "Epoch: 118/200, Step: 111000, D Loss: 1.1578365564346313, G Loss: 1.1782543659210205\n",
      "Epoch: 119/200, Step: 112000, D Loss: 1.1666350364685059, G Loss: 1.0070700645446777\n",
      "Epoch: 120/200, Step: 113000, D Loss: 1.241310954093933, G Loss: 1.2442128658294678\n",
      "Epoch: 121/200, Step: 114000, D Loss: 1.0589230060577393, G Loss: 1.0696357488632202\n",
      "Epoch: 122/200, Step: 115000, D Loss: 1.246326208114624, G Loss: 1.0027120113372803\n",
      "Epoch: 123/200, Step: 116000, D Loss: 1.1648263931274414, G Loss: 1.0870614051818848\n",
      "Epoch: 124/200, Step: 117000, D Loss: 1.079364538192749, G Loss: 1.0751209259033203\n",
      "Epoch: 125/200, Step: 118000, D Loss: 1.3040549755096436, G Loss: 1.1222002506256104\n",
      "Epoch: 127/200, Step: 119000, D Loss: 1.1919034719467163, G Loss: 0.9569960832595825\n",
      "Epoch: 128/200, Step: 120000, D Loss: 1.3699055910110474, G Loss: 1.0503509044647217\n",
      "Epoch: 129/200, Step: 121000, D Loss: 1.2357852458953857, G Loss: 0.9147998094558716\n",
      "Epoch: 130/200, Step: 122000, D Loss: 1.1771724224090576, G Loss: 0.8728904724121094\n",
      "Epoch: 131/200, Step: 123000, D Loss: 1.1884183883666992, G Loss: 1.068453073501587\n",
      "Epoch: 132/200, Step: 124000, D Loss: 1.1907727718353271, G Loss: 0.9228805303573608\n",
      "Epoch: 133/200, Step: 125000, D Loss: 1.2588005065917969, G Loss: 0.841820478439331\n",
      "Epoch: 134/200, Step: 126000, D Loss: 1.1472845077514648, G Loss: 1.0368342399597168\n",
      "Epoch: 135/200, Step: 127000, D Loss: 1.1513071060180664, G Loss: 1.030149221420288\n",
      "Epoch: 136/200, Step: 128000, D Loss: 1.13894784450531, G Loss: 0.924605131149292\n",
      "Epoch: 137/200, Step: 129000, D Loss: 1.3953145742416382, G Loss: 1.0721455812454224\n",
      "Epoch: 138/200, Step: 130000, D Loss: 1.1381340026855469, G Loss: 0.787496030330658\n",
      "Epoch: 139/200, Step: 131000, D Loss: 1.1472452878952026, G Loss: 1.0458474159240723\n",
      "Epoch: 140/200, Step: 132000, D Loss: 1.2100605964660645, G Loss: 1.049516201019287\n",
      "Epoch: 141/200, Step: 133000, D Loss: 1.224621057510376, G Loss: 1.0052416324615479\n",
      "Epoch: 143/200, Step: 134000, D Loss: 1.1528010368347168, G Loss: 0.934592068195343\n",
      "Epoch: 144/200, Step: 135000, D Loss: 1.04163658618927, G Loss: 1.1165392398834229\n",
      "Epoch: 145/200, Step: 136000, D Loss: 1.011618733406067, G Loss: 1.0907745361328125\n",
      "Epoch: 146/200, Step: 137000, D Loss: 0.9751995801925659, G Loss: 1.1560485363006592\n",
      "Epoch: 147/200, Step: 138000, D Loss: 1.2018687725067139, G Loss: 0.8908312320709229\n",
      "Epoch: 148/200, Step: 139000, D Loss: 1.1438100337982178, G Loss: 0.9689441323280334\n",
      "Epoch: 149/200, Step: 140000, D Loss: 1.214618444442749, G Loss: 0.9717847108840942\n",
      "Epoch: 150/200, Step: 141000, D Loss: 1.167806625366211, G Loss: 1.1796717643737793\n",
      "Epoch: 151/200, Step: 142000, D Loss: 1.2575979232788086, G Loss: 0.9662413597106934\n",
      "Epoch: 152/200, Step: 143000, D Loss: 1.1783664226531982, G Loss: 1.0042452812194824\n",
      "Epoch: 153/200, Step: 144000, D Loss: 1.1053972244262695, G Loss: 1.0665063858032227\n",
      "Epoch: 154/200, Step: 145000, D Loss: 1.2829701900482178, G Loss: 0.8794151544570923\n",
      "Epoch: 155/200, Step: 146000, D Loss: 1.215099811553955, G Loss: 1.1880245208740234\n",
      "Epoch: 156/200, Step: 147000, D Loss: 1.2347040176391602, G Loss: 1.0011544227600098\n",
      "Epoch: 157/200, Step: 148000, D Loss: 1.1383966207504272, G Loss: 0.9621474742889404\n",
      "Epoch: 159/200, Step: 149000, D Loss: 1.252417802810669, G Loss: 1.074437141418457\n",
      "Epoch: 160/200, Step: 150000, D Loss: 1.2481974363327026, G Loss: 1.4966673851013184\n",
      "Epoch: 161/200, Step: 151000, D Loss: 1.1966540813446045, G Loss: 0.8049484491348267\n",
      "Epoch: 162/200, Step: 152000, D Loss: 1.2606462240219116, G Loss: 0.8686121702194214\n",
      "Epoch: 163/200, Step: 153000, D Loss: 1.1873271465301514, G Loss: 0.9761514663696289\n",
      "Epoch: 164/200, Step: 154000, D Loss: 1.1541621685028076, G Loss: 0.8560274839401245\n",
      "Epoch: 165/200, Step: 155000, D Loss: 1.3041703701019287, G Loss: 0.8098909854888916\n",
      "Epoch: 166/200, Step: 156000, D Loss: 1.1193022727966309, G Loss: 1.0280277729034424\n",
      "Epoch: 167/200, Step: 157000, D Loss: 1.100337266921997, G Loss: 1.0236616134643555\n",
      "Epoch: 168/200, Step: 158000, D Loss: 1.2838916778564453, G Loss: 0.9101704359054565\n",
      "Epoch: 169/200, Step: 159000, D Loss: 1.1329158544540405, G Loss: 1.285172462463379\n",
      "Epoch: 170/200, Step: 160000, D Loss: 1.255687952041626, G Loss: 0.9020231366157532\n",
      "Epoch: 171/200, Step: 161000, D Loss: 1.184150218963623, G Loss: 1.0478074550628662\n",
      "Epoch: 172/200, Step: 162000, D Loss: 1.1776694059371948, G Loss: 1.1878951787948608\n",
      "Epoch: 173/200, Step: 163000, D Loss: 1.1962509155273438, G Loss: 1.150924563407898\n",
      "Epoch: 175/200, Step: 164000, D Loss: 1.1679662466049194, G Loss: 0.9586357474327087\n",
      "Epoch: 176/200, Step: 165000, D Loss: 1.329978108406067, G Loss: 0.7934539318084717\n",
      "Epoch: 177/200, Step: 166000, D Loss: 1.1629115343093872, G Loss: 0.9149640798568726\n",
      "Epoch: 178/200, Step: 167000, D Loss: 1.202217936515808, G Loss: 1.0081260204315186\n",
      "Epoch: 179/200, Step: 168000, D Loss: 1.2779453992843628, G Loss: 1.0519870519638062\n",
      "Epoch: 180/200, Step: 169000, D Loss: 1.2658171653747559, G Loss: 1.0021659135818481\n",
      "Epoch: 181/200, Step: 170000, D Loss: 1.157170057296753, G Loss: 1.0540385246276855\n",
      "Epoch: 182/200, Step: 171000, D Loss: 1.1179475784301758, G Loss: 1.1391522884368896\n",
      "Epoch: 183/200, Step: 172000, D Loss: 1.2381008863449097, G Loss: 1.0050482749938965\n",
      "Epoch: 184/200, Step: 173000, D Loss: 1.1574760675430298, G Loss: 1.147648572921753\n",
      "Epoch: 185/200, Step: 174000, D Loss: 1.1090824604034424, G Loss: 1.2275454998016357\n",
      "Epoch: 186/200, Step: 175000, D Loss: 1.1387372016906738, G Loss: 0.9123997688293457\n",
      "Epoch: 187/200, Step: 176000, D Loss: 1.1860710382461548, G Loss: 1.1013376712799072\n",
      "Epoch: 188/200, Step: 177000, D Loss: 1.1705490350723267, G Loss: 1.0290288925170898\n",
      "Epoch: 189/200, Step: 178000, D Loss: 1.2190825939178467, G Loss: 0.8836700916290283\n",
      "Epoch: 191/200, Step: 179000, D Loss: 1.1486271619796753, G Loss: 1.3457155227661133\n",
      "Epoch: 192/200, Step: 180000, D Loss: 1.0046473741531372, G Loss: 1.068560004234314\n",
      "Epoch: 193/200, Step: 181000, D Loss: 1.2534759044647217, G Loss: 1.4762152433395386\n",
      "Epoch: 194/200, Step: 182000, D Loss: 1.2101258039474487, G Loss: 0.9018652439117432\n",
      "Epoch: 195/200, Step: 183000, D Loss: 1.1107065677642822, G Loss: 0.941300630569458\n",
      "Epoch: 196/200, Step: 184000, D Loss: 1.0906507968902588, G Loss: 1.2708215713500977\n",
      "Epoch: 197/200, Step: 185000, D Loss: 1.2352685928344727, G Loss: 1.0589549541473389\n",
      "Epoch: 198/200, Step: 186000, D Loss: 1.147063136100769, G Loss: 0.844939112663269\n",
      "Epoch: 199/200, Step: 187000, D Loss: 1.1743619441986084, G Loss: 0.9641535878181458\n"
     ]
    }
   ],
   "source": [
    "D_labels = to_cuda(torch.ones(batch_size)) # Discriminator Label to real\n",
    "D_fakes = to_cuda(torch.zeros(batch_size)) # Discriminator Label to fake\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for idx, (images, labels) in enumerate(data_loader):\n",
    "        step += 1\n",
    "        # Training Discriminator\n",
    "        x = to_cuda(images)\n",
    "        y = to_cuda(labels)\n",
    "        y = y.view(batch_size, 1)\n",
    "        y = to_onehot(y)\n",
    "        x_outputs = D(x, y)\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "\n",
    "        z = to_cuda(torch.randn(batch_size, n_noise))\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        D_z_loss = criterion(z_outputs, D_fakes)\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "        \n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "\n",
    "        if step % n_critic == 0:\n",
    "            # Training Generator\n",
    "            z = to_cuda(torch.randn(batch_size, n_noise))\n",
    "            z_outputs = D(G(z, y), y)\n",
    "            G_loss = criterion(z_outputs, D_labels)\n",
    "\n",
    "            D.zero_grad()\n",
    "            G.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_opt.step()\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {}'.format(epoch, max_epoch, step, D_loss.data[0], G_loss.data[0]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, file_name='checkpoint.pth.tar'):\n",
    "    torch.save(state, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving params.\n",
    "# torch.save(D.state_dict(), 'D_c.pkl')\n",
    "# torch.save(G.state_dict(), 'G_c.pkl')\n",
    "save_checkpoint({'epoch': epoch + 1, 'state_dict':D.state_dict(), 'optimizer' : D_opt.state_dict()}, 'D_c.pth.tar')\n",
    "save_checkpoint({'epoch': epoch + 1, 'state_dict':G.state_dict(), 'optimizer' : G_opt.state_dict()}, 'G_c.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyangii/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB/klEQVR4nF2SP2hUQRCHv5m37+X+\nJsGcF0HPiEJiY7CwEkUMpgjYiYQg1oJgYSf2EkGw1caolYWgQVBQkGBlk2Bl1MIqYJJGT5NcLrm3\nOxb3XjwcWHaZZXbn9/0GoQKgRACIIAhZCJSFqDchAFWkgIgAWV1+kv9WHgkyE3XLHaLU8gvtebeS\n5wRgQKgL4IoAGkFVgaIgSKtRH204Gb4WCxB8zAZSBKdOzvUPnn81d3l49N2dBICS4GwbvKgdSWrX\n7eyUDcUjn156oBV5QFEZv/pxen3Xm6VmnYkEjlEWzYRPfP75/EkrWDCzzt0DKu5f96WLR5fHmg+X\nf79Jsa8v1hHflS/AyKTfvnD81unq7aalc7FmXBRDcbbBvRu7i1tz/dhbtdIeMCGpjDzbTP38WOOP\nmV+rRwO1CJEkw9aY7Zj59i+z4Ds3C1fG+zJ0QiBtv97ZbsUDWPCbpw4uVlK0yx7kxP7pk276Wzts\nrX1fOFPWbkMiBiCO1LQWLw0uLS8Un5IiBmoCYBKM0F6d/HE4ulQPHqzHxIICqo3F5sqXccllZAAj\nQUS0tJ76zrDLje5+KYEIs9B5EGTnUR8aAeJMTIMBHoBDno3ZHSy4FFOMsAdLh5or99/PPE6MtDuJ\n0jOtydi++dWpKEYzU0RRIFKgCvJBXAwiudma7Y7cRgHkL7BEsXViPWytAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5E41D1710>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generation to image\n",
    "z = to_cuda(torch.randn(1, n_noise))\n",
    "y = to_onehot(9)\n",
    "fakeimg = G(z, y).view(28, 28)\n",
    "img = fakeimg.cpu().data.numpy()\n",
    "scipy.misc.toimage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
