{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴럴넷과 CNN에 대해 기본적인 강의를 듣고 기초지식을 가지고 있다는 가정하에 진행되는 튜토리얼입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' # for using only cpu\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([2],dtype=tf.float32)\n",
    "b = tf.constant([3],dtype=tf.float32)\n",
    "c = tf.constant([4],dtype=tf.float32)\n",
    "\n",
    "op = a*b+c\n",
    "# op = tf.add(tf.multiply(a,b), c)\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(op)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7]\n",
      "[[-1. -1. -1.]\n",
      " [-1. -1. -1.]]\n",
      "Tensor(\"Const_3:0\", shape=(7,), dtype=int32)\n",
      "Tensor(\"Const_4:0\", shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Constant 1-D Tensor populated with value list.\n",
    "tensor1 = tf.constant([1, 2, 3, 4, 5, 6, 7]) # [1 2 3 4 5 6 7]\n",
    "\n",
    "# Constant 2-D tensor populated with scalar value -1.\n",
    "tensor2 = tf.constant(-1.0, shape=[2, 3]) # [[-1. -1. -1.], [-1. -1. -1.]]\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(tensor1))\n",
    "print(sess.run(tensor2))\n",
    "print(tensor1)\n",
    "print(tensor2)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "input_data = [1,2,3,4,5]\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "k = tf.constant(2, dtype=tf.float32)\n",
    "y = x * k\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(y, feed_dict={x:input_data})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  20.   40.   60.   80.  100.]\n"
     ]
    }
   ],
   "source": [
    "input_data = [1,2,3,4,5]\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "W = tf.Variable([10],dtype=tf.float32)\n",
    "W = W.assign_add([10.0])\n",
    "y = W*x\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "result = sess.run(y, feed_dict={x:input_data})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST w/ Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-7-df9e97748cf2>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAhElEQVR4nNWRSxICIQxEX1nea+LJ\njCcLnqxdwAAjnyqX9pJHd9IA/yt3SWFTpCIfWaiqnNwrE8ArQYCli81CUh7WnP28zKwNvXUXnjkC\nOK5OK3t6v9A3lWr+vEqcJ60KD8wOA96jsWbHnOXkHfNN6IKxqgHlxxaM2ITiQ4vu4dOy/u/6AHLl\nduTLFPT8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x190C03BD048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.misc.toimage(mnist.train.images[999].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([784, 10], stddev=0.1))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = tf.nn.softmax(tf.matmul(images, W) + b)\n",
    "loss = -tf.reduce_sum(labels*tf.log(logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_op, feed_dict={images: batch_xs, labels: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logit, axis=1), tf.argmax(labels, axis=1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89910907\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(acc, feed_dict={images: mnist.train.images, labels: mnist.train.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9032\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(acc, feed_dict={images: mnist.test.images, labels: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "# W = tf.Variable(tf.zeros([784, 10]))\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 100], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([100]))\n",
    "W2 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out1 = tf.nn.relu(tf.matmul(images, W1) + b1)\n",
    "logit = tf.matmul(out1, W2) + b2\n",
    "# loss = -tf.reduce_sum(labels*tf.log(logit))\n",
    "# loss = tf.nn.softmax_cross_entropy_with_logits()\n",
    "loss = tf.reduce_sum(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logit)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146.492\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100, shuffle=True)\n",
    "    _, batch_loss = sess.run([train_op, loss], feed_dict={images: batch_xs, labels: batch_ys})\n",
    "print(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = tf.reduce_mean(tf.cast(\n",
    "    tf.equal(tf.argmax(logit, axis=1), tf.argmax(labels, axis=1)),\n",
    "    tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.991273\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(acc, feed_dict={images: mnist.train.images, labels: mnist.train.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9769\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(acc, feed_dict={images: mnist.test.images, labels: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습! 성능 높이기: 15분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST w/ CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "reshape_images = tf.reshape(images, [-1, 28, 28, 1])\n",
    "\n",
    "labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layer1 = tf.nn.relu(conv(reshape_images, W_conv1) + b_conv1)\n",
    "pool_layer1 = max_pool(conv_layer1)\n",
    "\n",
    "conv_layer2 = tf.nn.relu(conv(pool_layer1, W_conv2) + b_conv2)\n",
    "pool_layer2 = max_pool(conv_layer2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool_layer2, [-1, 7*7*64])\n",
    "fc_layer1 = tf.nn.relu(tf.matmul(pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "drop_out = tf.nn.dropout(fc_layer1, keep_prob)\n",
    "\n",
    "logit = tf.matmul(drop_out, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-8bbe37ab3257>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "step 0, training accuracy 0.08\n",
      "test accuracy 0.068\n",
      "step 1, training accuracy 0.16\n",
      "step 2, training accuracy 0.08\n",
      "step 3, training accuracy 0.14\n",
      "step 4, training accuracy 0.16\n",
      "step 5, training accuracy 0.22\n",
      "step 6, training accuracy 0.14\n",
      "step 7, training accuracy 0.1\n",
      "step 8, training accuracy 0.18\n",
      "step 9, training accuracy 0.22\n",
      "step 10, training accuracy 0.18\n",
      "step 11, training accuracy 0.18\n",
      "step 12, training accuracy 0.18\n",
      "step 13, training accuracy 0.2\n",
      "step 14, training accuracy 0.24\n",
      "step 15, training accuracy 0.3\n",
      "step 16, training accuracy 0.28\n",
      "step 17, training accuracy 0.3\n",
      "step 18, training accuracy 0.36\n",
      "step 19, training accuracy 0.36\n",
      "step 20, training accuracy 0.34\n",
      "step 21, training accuracy 0.3\n",
      "step 22, training accuracy 0.52\n",
      "step 23, training accuracy 0.46\n",
      "step 24, training accuracy 0.3\n",
      "step 25, training accuracy 0.42\n",
      "step 26, training accuracy 0.48\n",
      "step 27, training accuracy 0.44\n",
      "step 28, training accuracy 0.52\n",
      "step 29, training accuracy 0.54\n",
      "step 30, training accuracy 0.52\n",
      "step 31, training accuracy 0.46\n",
      "step 32, training accuracy 0.66\n",
      "step 33, training accuracy 0.64\n",
      "step 34, training accuracy 0.6\n",
      "step 35, training accuracy 0.56\n",
      "step 36, training accuracy 0.52\n",
      "step 37, training accuracy 0.62\n",
      "step 38, training accuracy 0.64\n",
      "step 39, training accuracy 0.56\n",
      "step 40, training accuracy 0.7\n",
      "step 41, training accuracy 0.58\n",
      "step 42, training accuracy 0.62\n",
      "step 43, training accuracy 0.62\n",
      "step 44, training accuracy 0.52\n",
      "step 45, training accuracy 0.78\n",
      "step 46, training accuracy 0.76\n",
      "step 47, training accuracy 0.76\n",
      "step 48, training accuracy 0.7\n",
      "step 49, training accuracy 0.78\n",
      "step 50, training accuracy 0.7\n",
      "step 51, training accuracy 0.78\n",
      "step 52, training accuracy 0.6\n",
      "step 53, training accuracy 0.82\n",
      "step 54, training accuracy 0.76\n",
      "step 55, training accuracy 0.7\n",
      "step 56, training accuracy 0.78\n",
      "step 57, training accuracy 0.74\n",
      "step 58, training accuracy 0.8\n",
      "step 59, training accuracy 0.72\n",
      "step 60, training accuracy 0.7\n",
      "step 61, training accuracy 0.74\n",
      "step 62, training accuracy 0.72\n",
      "step 63, training accuracy 0.7\n",
      "step 64, training accuracy 0.72\n",
      "step 65, training accuracy 0.74\n",
      "step 66, training accuracy 0.8\n",
      "step 67, training accuracy 0.74\n",
      "step 68, training accuracy 0.72\n",
      "step 69, training accuracy 0.68\n",
      "step 70, training accuracy 0.8\n",
      "step 71, training accuracy 0.78\n",
      "step 72, training accuracy 0.76\n",
      "step 73, training accuracy 0.84\n",
      "step 74, training accuracy 0.9\n",
      "step 75, training accuracy 0.78\n",
      "step 76, training accuracy 0.78\n",
      "step 77, training accuracy 0.84\n",
      "step 78, training accuracy 0.82\n",
      "step 79, training accuracy 0.8\n",
      "step 80, training accuracy 0.78\n",
      "step 81, training accuracy 0.9\n",
      "step 82, training accuracy 0.78\n",
      "step 83, training accuracy 0.8\n",
      "step 84, training accuracy 0.84\n",
      "step 85, training accuracy 0.78\n",
      "step 86, training accuracy 0.82\n",
      "step 87, training accuracy 0.74\n",
      "step 88, training accuracy 0.8\n",
      "step 89, training accuracy 0.8\n",
      "step 90, training accuracy 0.86\n",
      "step 91, training accuracy 0.82\n",
      "step 92, training accuracy 0.78\n",
      "step 93, training accuracy 0.84\n",
      "step 94, training accuracy 0.84\n",
      "step 95, training accuracy 0.82\n",
      "step 96, training accuracy 0.88\n",
      "step 97, training accuracy 0.86\n",
      "step 98, training accuracy 0.92\n",
      "step 99, training accuracy 0.86\n",
      "step 100, training accuracy 0.8\n",
      "test accuracy 0.8545\n",
      "step 101, training accuracy 0.78\n",
      "step 102, training accuracy 0.8\n",
      "step 103, training accuracy 0.82\n",
      "step 104, training accuracy 0.96\n",
      "step 105, training accuracy 0.86\n",
      "step 106, training accuracy 0.76\n",
      "step 107, training accuracy 0.84\n",
      "step 108, training accuracy 0.8\n",
      "step 109, training accuracy 0.88\n",
      "step 110, training accuracy 0.78\n",
      "step 111, training accuracy 0.76\n",
      "step 112, training accuracy 0.76\n",
      "step 113, training accuracy 0.86\n",
      "step 114, training accuracy 0.82\n",
      "step 115, training accuracy 0.76\n",
      "step 116, training accuracy 0.86\n",
      "step 117, training accuracy 0.82\n",
      "step 118, training accuracy 0.74\n",
      "step 119, training accuracy 0.84\n",
      "step 120, training accuracy 0.82\n",
      "step 121, training accuracy 0.8\n",
      "step 122, training accuracy 0.8\n",
      "step 123, training accuracy 0.88\n",
      "step 124, training accuracy 0.86\n",
      "step 125, training accuracy 0.9\n",
      "step 126, training accuracy 0.94\n",
      "step 127, training accuracy 0.98\n",
      "step 128, training accuracy 0.86\n",
      "step 129, training accuracy 0.94\n",
      "step 130, training accuracy 0.88\n",
      "step 131, training accuracy 0.9\n",
      "step 132, training accuracy 0.84\n",
      "step 133, training accuracy 0.82\n",
      "step 134, training accuracy 0.94\n",
      "step 135, training accuracy 0.92\n",
      "step 136, training accuracy 0.88\n",
      "step 137, training accuracy 0.92\n",
      "step 138, training accuracy 0.96\n",
      "step 139, training accuracy 0.88\n",
      "step 140, training accuracy 0.88\n",
      "step 141, training accuracy 0.9\n",
      "step 142, training accuracy 0.84\n",
      "step 143, training accuracy 0.94\n",
      "step 144, training accuracy 0.9\n",
      "step 145, training accuracy 0.9\n",
      "step 146, training accuracy 0.8\n",
      "step 147, training accuracy 0.74\n",
      "step 148, training accuracy 0.82\n",
      "step 149, training accuracy 0.88\n",
      "step 150, training accuracy 0.86\n",
      "step 151, training accuracy 0.92\n",
      "step 152, training accuracy 0.94\n",
      "step 153, training accuracy 0.86\n",
      "step 154, training accuracy 0.86\n",
      "step 155, training accuracy 0.82\n",
      "step 156, training accuracy 0.82\n",
      "step 157, training accuracy 0.86\n",
      "step 158, training accuracy 0.96\n",
      "step 159, training accuracy 0.82\n",
      "step 160, training accuracy 0.84\n",
      "step 161, training accuracy 0.9\n",
      "step 162, training accuracy 0.86\n",
      "step 163, training accuracy 0.88\n",
      "step 164, training accuracy 0.92\n",
      "step 165, training accuracy 0.84\n",
      "step 166, training accuracy 0.9\n",
      "step 167, training accuracy 0.86\n",
      "step 168, training accuracy 0.84\n",
      "step 169, training accuracy 0.92\n",
      "step 170, training accuracy 0.96\n",
      "step 171, training accuracy 0.9\n",
      "step 172, training accuracy 0.9\n",
      "step 173, training accuracy 0.88\n",
      "step 174, training accuracy 0.88\n",
      "step 175, training accuracy 0.9\n",
      "step 176, training accuracy 0.98\n",
      "step 177, training accuracy 0.86\n",
      "step 178, training accuracy 0.88\n",
      "step 179, training accuracy 0.9\n",
      "step 180, training accuracy 0.92\n",
      "step 181, training accuracy 0.88\n",
      "step 182, training accuracy 0.92\n",
      "step 183, training accuracy 0.94\n",
      "step 184, training accuracy 0.94\n",
      "step 185, training accuracy 0.88\n",
      "step 186, training accuracy 0.94\n",
      "step 187, training accuracy 0.88\n",
      "step 188, training accuracy 0.94\n",
      "step 189, training accuracy 0.94\n",
      "step 190, training accuracy 0.92\n",
      "step 191, training accuracy 0.9\n",
      "step 192, training accuracy 0.84\n",
      "step 193, training accuracy 0.88\n",
      "step 194, training accuracy 0.9\n",
      "step 195, training accuracy 0.92\n",
      "step 196, training accuracy 0.9\n",
      "step 197, training accuracy 0.98\n",
      "step 198, training accuracy 0.92\n",
      "step 199, training accuracy 0.94\n",
      "step 200, training accuracy 0.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8bbe37ab3257>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             test_acc = sess.run(accuracy, \n\u001b[1;32m---> 28\u001b[1;33m                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             )\n\u001b[0;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test accuracy %g'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yangyangii\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logit)\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=labels)\n",
    ")\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(logit, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "max_epoch = 1\n",
    "total_batch = int(mnist.train.num_examples/BATCH_SIZE)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(max_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch = mnist.train.next_batch(BATCH_SIZE, shuffle=True)\n",
    "        sess.run(train_op, \n",
    "             feed_dict={images: batch[0], labels: batch[1], keep_prob: 0.5}\n",
    "        )\n",
    "        train_acc = sess.run(accuracy,\n",
    "            feed_dict={images: batch[0], labels: batch[1], keep_prob: 1.0}\n",
    "        )\n",
    "        print('step %d, training accuracy %g' % (i, train_acc))\n",
    "        if i % 100 == 0:\n",
    "            test_acc = sess.run(accuracy, \n",
    "                 feed_dict={images: mnist.test.images, labels: mnist.test.labels, keep_prob: 1.0}\n",
    "            )\n",
    "            print('test accuracy %g' % test_acc)\n",
    "test_acc = sess.run(accuracy, \n",
    "     feed_dict={images: mnist.test.images, labels: mnist.test.labels, keep_prob: 1.0}\n",
    ")\n",
    "print('test accuracy %g' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습! 성능 높이기: 15분, epoch 1000 이내"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction w/ CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습! Prediction 해보기: 10분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9klEQVR4nGNgoD9ghDG0rHUZgl/r\nX2Q4W/3zPZoilkd/YeDxKUk0ydS/f3//+DGl2zxi6Y+/wWiSRd8OmcPYX24IQ42DCqy+9v4kXOXP\n3zjdqPfjvjiExYQpacJ64iUOfWaHbx4WxiHX9fnvMT4ccsp7Hq0Twi6lNvPV/jIkPguyJLv0v2Ur\ncZipOsMUVQAe8AxeEg/vPsChzezTu3B0MZjOdLFnB+6iS0IcpLD5y9UrGHJQnUqXORj+nzrOcElv\ntehuSf+zq4R6qhFqwq/Dovr76v27//79e4obyU5B9UgGBj2zj6u/b9e88zeA4fGkXzhcTmMAAL7m\nZmt4PmtlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F3D51AD2DD8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = mnist.train.images[999:1000]\n",
    "print(np.argmax(mnist.train.labels[999]))\n",
    "scipy.misc.toimage(input_data[0].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Result: 2\n"
     ]
    }
   ],
   "source": [
    "pred = tf.argmax(logit, 1)\n",
    "result = sess.run(pred, feed_dict={images: input_data, keep_prob: 1.0})\n",
    "print('Prediction Result: {}'.format(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자 2에 대한 이미지를 예측하고 결과를 보여주는 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard w/ CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name='weights'):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name='biases'):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(x, W, name='conv'):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x, name='max_pool'):\n",
    "    with tf.name_scope(name) as scope:\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "reshape_images = tf.reshape(images, [-1, 28, 28, 1])\n",
    "\n",
    "labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layer1 = tf.nn.relu(conv(reshape_images, W_conv1) + b_conv1)\n",
    "pool_layer1 = max_pool(conv_layer1)\n",
    "\n",
    "conv_layer2 = tf.nn.relu(conv(pool_layer1, W_conv2) + b_conv2)\n",
    "pool_layer2 = max_pool(conv_layer2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool_layer2, [-1, 7*7*64])\n",
    "\n",
    "with tf.name_scope('fc_layer') as scope:\n",
    "    fc_layer1 = tf.nn.relu(tf.matmul(pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "with tf.name_scope('fc_layer') as scope:\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    drop_out = tf.nn.dropout(fc_layer1, keep_prob)\n",
    "    logit = tf.matmul(drop_out, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "test accuracy 0.1019\n",
      "step 1, training accuracy 0.06\n",
      "step 2, training accuracy 0.08\n",
      "step 3, training accuracy 0.06\n",
      "step 4, training accuracy 0.12\n",
      "step 5, training accuracy 0.14\n",
      "step 6, training accuracy 0.2\n",
      "step 7, training accuracy 0.16\n",
      "step 8, training accuracy 0.16\n",
      "step 9, training accuracy 0.22\n",
      "step 10, training accuracy 0.2\n",
      "step 11, training accuracy 0.24\n",
      "step 12, training accuracy 0.2\n",
      "step 13, training accuracy 0.32\n",
      "step 14, training accuracy 0.26\n",
      "step 15, training accuracy 0.3\n",
      "step 16, training accuracy 0.4\n",
      "step 17, training accuracy 0.4\n",
      "step 18, training accuracy 0.56\n",
      "step 19, training accuracy 0.4\n",
      "step 20, training accuracy 0.4\n",
      "step 21, training accuracy 0.56\n",
      "step 22, training accuracy 0.52\n",
      "step 23, training accuracy 0.44\n",
      "step 24, training accuracy 0.5\n",
      "step 25, training accuracy 0.52\n",
      "step 26, training accuracy 0.56\n",
      "step 27, training accuracy 0.44\n",
      "step 28, training accuracy 0.46\n",
      "step 29, training accuracy 0.64\n",
      "step 30, training accuracy 0.5\n",
      "step 31, training accuracy 0.56\n",
      "step 32, training accuracy 0.64\n",
      "step 33, training accuracy 0.58\n",
      "step 34, training accuracy 0.64\n",
      "step 35, training accuracy 0.58\n",
      "step 36, training accuracy 0.66\n",
      "step 37, training accuracy 0.64\n",
      "step 38, training accuracy 0.6\n",
      "step 39, training accuracy 0.66\n",
      "step 40, training accuracy 0.76\n",
      "step 41, training accuracy 0.82\n",
      "step 42, training accuracy 0.68\n",
      "step 43, training accuracy 0.64\n",
      "step 44, training accuracy 0.66\n",
      "step 45, training accuracy 0.64\n",
      "step 46, training accuracy 0.78\n",
      "step 47, training accuracy 0.68\n",
      "step 48, training accuracy 0.72\n",
      "step 49, training accuracy 0.7\n",
      "step 50, training accuracy 0.66\n",
      "step 51, training accuracy 0.82\n",
      "step 52, training accuracy 0.7\n",
      "step 53, training accuracy 0.74\n",
      "step 54, training accuracy 0.84\n",
      "step 55, training accuracy 0.78\n",
      "step 56, training accuracy 0.72\n",
      "step 57, training accuracy 0.8\n",
      "step 58, training accuracy 0.78\n",
      "step 59, training accuracy 0.82\n",
      "step 60, training accuracy 0.8\n",
      "step 61, training accuracy 0.74\n",
      "step 62, training accuracy 0.74\n",
      "step 63, training accuracy 0.8\n",
      "step 64, training accuracy 0.76\n",
      "step 65, training accuracy 0.7\n",
      "step 66, training accuracy 0.72\n",
      "step 67, training accuracy 0.7\n",
      "step 68, training accuracy 0.78\n",
      "step 69, training accuracy 0.84\n",
      "step 70, training accuracy 0.8\n",
      "step 71, training accuracy 0.8\n",
      "step 72, training accuracy 0.74\n",
      "step 73, training accuracy 0.78\n",
      "step 74, training accuracy 0.78\n",
      "step 75, training accuracy 0.84\n",
      "step 76, training accuracy 0.82\n",
      "step 77, training accuracy 0.8\n",
      "step 78, training accuracy 0.84\n",
      "step 79, training accuracy 0.8\n",
      "step 80, training accuracy 0.84\n",
      "step 81, training accuracy 0.84\n",
      "step 82, training accuracy 0.88\n",
      "step 83, training accuracy 0.9\n",
      "step 84, training accuracy 0.76\n",
      "step 85, training accuracy 0.88\n",
      "step 86, training accuracy 0.84\n",
      "step 87, training accuracy 0.86\n",
      "step 88, training accuracy 0.76\n",
      "step 89, training accuracy 0.86\n",
      "step 90, training accuracy 0.88\n",
      "step 91, training accuracy 0.8\n",
      "step 92, training accuracy 0.84\n",
      "step 93, training accuracy 0.92\n",
      "step 94, training accuracy 0.8\n",
      "step 95, training accuracy 0.82\n",
      "step 96, training accuracy 0.8\n",
      "step 97, training accuracy 0.84\n",
      "step 98, training accuracy 0.84\n",
      "step 99, training accuracy 0.82\n",
      "step 100, training accuracy 0.82\n",
      "test accuracy 0.8508\n",
      "step 101, training accuracy 0.88\n",
      "step 102, training accuracy 0.78\n",
      "step 103, training accuracy 0.82\n",
      "step 104, training accuracy 0.84\n",
      "step 105, training accuracy 0.9\n",
      "step 106, training accuracy 0.82\n",
      "step 107, training accuracy 0.82\n",
      "step 108, training accuracy 0.8\n",
      "step 109, training accuracy 0.88\n",
      "step 110, training accuracy 0.92\n",
      "step 111, training accuracy 0.9\n",
      "step 112, training accuracy 0.86\n",
      "step 113, training accuracy 0.98\n",
      "step 114, training accuracy 0.78\n",
      "step 115, training accuracy 0.88\n",
      "step 116, training accuracy 0.8\n",
      "step 117, training accuracy 0.78\n",
      "step 118, training accuracy 0.9\n",
      "step 119, training accuracy 0.76\n",
      "step 120, training accuracy 0.84\n",
      "step 121, training accuracy 0.92\n",
      "step 122, training accuracy 0.9\n",
      "step 123, training accuracy 0.88\n",
      "step 124, training accuracy 0.88\n",
      "step 125, training accuracy 0.88\n",
      "step 126, training accuracy 0.9\n",
      "step 127, training accuracy 0.86\n",
      "step 128, training accuracy 0.86\n",
      "step 129, training accuracy 0.94\n",
      "step 130, training accuracy 0.84\n",
      "step 131, training accuracy 0.88\n",
      "step 132, training accuracy 0.74\n",
      "step 133, training accuracy 0.92\n",
      "step 134, training accuracy 0.86\n",
      "step 135, training accuracy 0.92\n",
      "step 136, training accuracy 0.92\n",
      "step 137, training accuracy 0.86\n",
      "step 138, training accuracy 0.9\n",
      "step 139, training accuracy 0.88\n",
      "step 140, training accuracy 0.82\n",
      "step 141, training accuracy 0.84\n",
      "step 142, training accuracy 0.82\n",
      "step 143, training accuracy 0.82\n",
      "step 144, training accuracy 0.88\n",
      "step 145, training accuracy 0.86\n",
      "step 146, training accuracy 0.82\n",
      "step 147, training accuracy 0.88\n",
      "step 148, training accuracy 0.94\n",
      "step 149, training accuracy 0.9\n",
      "step 150, training accuracy 0.84\n",
      "step 151, training accuracy 0.86\n",
      "step 152, training accuracy 0.88\n",
      "step 153, training accuracy 0.88\n",
      "step 154, training accuracy 0.86\n",
      "step 155, training accuracy 0.92\n",
      "step 156, training accuracy 0.94\n",
      "step 157, training accuracy 0.92\n",
      "step 158, training accuracy 0.9\n",
      "step 159, training accuracy 0.82\n",
      "step 160, training accuracy 0.9\n",
      "step 161, training accuracy 0.92\n",
      "step 162, training accuracy 0.9\n",
      "step 163, training accuracy 0.82\n",
      "step 164, training accuracy 0.92\n",
      "step 165, training accuracy 0.9\n",
      "step 166, training accuracy 0.84\n",
      "step 167, training accuracy 0.92\n",
      "step 168, training accuracy 0.84\n",
      "step 169, training accuracy 0.94\n",
      "step 170, training accuracy 0.96\n",
      "step 171, training accuracy 0.88\n",
      "step 172, training accuracy 0.92\n",
      "step 173, training accuracy 0.96\n",
      "step 174, training accuracy 0.94\n",
      "step 175, training accuracy 0.9\n",
      "step 176, training accuracy 0.82\n",
      "step 177, training accuracy 0.88\n",
      "step 178, training accuracy 0.92\n",
      "step 179, training accuracy 0.88\n",
      "step 180, training accuracy 0.96\n",
      "step 181, training accuracy 0.86\n",
      "step 182, training accuracy 0.8\n",
      "step 183, training accuracy 0.92\n",
      "step 184, training accuracy 0.94\n",
      "step 185, training accuracy 0.92\n",
      "step 186, training accuracy 0.88\n",
      "step 187, training accuracy 0.92\n",
      "step 188, training accuracy 0.94\n",
      "step 189, training accuracy 0.94\n",
      "step 190, training accuracy 0.94\n",
      "step 191, training accuracy 0.92\n",
      "step 192, training accuracy 0.94\n",
      "step 193, training accuracy 0.88\n",
      "step 194, training accuracy 0.9\n",
      "step 195, training accuracy 0.84\n",
      "step 196, training accuracy 0.92\n",
      "step 197, training accuracy 0.86\n",
      "step 198, training accuracy 0.88\n",
      "step 199, training accuracy 0.86\n",
      "step 200, training accuracy 0.92\n",
      "test accuracy 0.9001\n",
      "step 201, training accuracy 0.8\n",
      "step 202, training accuracy 0.82\n",
      "step 203, training accuracy 0.96\n",
      "step 204, training accuracy 0.94\n",
      "step 205, training accuracy 0.94\n",
      "step 206, training accuracy 0.86\n",
      "step 207, training accuracy 0.88\n",
      "step 208, training accuracy 0.9\n",
      "step 209, training accuracy 0.94\n",
      "step 210, training accuracy 0.98\n",
      "step 211, training accuracy 0.82\n",
      "step 212, training accuracy 0.9\n",
      "step 213, training accuracy 0.94\n",
      "step 214, training accuracy 0.94\n",
      "step 215, training accuracy 0.88\n",
      "step 216, training accuracy 0.94\n",
      "step 217, training accuracy 1\n",
      "step 218, training accuracy 0.94\n",
      "step 219, training accuracy 0.86\n",
      "step 220, training accuracy 0.88\n",
      "step 221, training accuracy 0.96\n",
      "step 222, training accuracy 0.92\n",
      "step 223, training accuracy 0.96\n",
      "step 224, training accuracy 0.92\n",
      "step 225, training accuracy 0.92\n",
      "step 226, training accuracy 0.94\n",
      "step 227, training accuracy 0.94\n",
      "step 228, training accuracy 0.92\n",
      "step 229, training accuracy 0.84\n",
      "step 230, training accuracy 0.92\n",
      "step 231, training accuracy 0.9\n",
      "step 232, training accuracy 0.94\n",
      "step 233, training accuracy 0.9\n",
      "step 234, training accuracy 0.92\n",
      "step 235, training accuracy 0.94\n",
      "step 236, training accuracy 0.86\n",
      "step 237, training accuracy 0.9\n",
      "step 238, training accuracy 0.92\n",
      "step 239, training accuracy 0.9\n",
      "step 240, training accuracy 0.9\n",
      "step 241, training accuracy 0.96\n",
      "step 242, training accuracy 0.9\n",
      "step 243, training accuracy 0.82\n",
      "step 244, training accuracy 0.98\n",
      "step 245, training accuracy 0.98\n",
      "step 246, training accuracy 0.92\n",
      "step 247, training accuracy 0.94\n",
      "step 248, training accuracy 0.86\n",
      "step 249, training accuracy 0.78\n",
      "step 250, training accuracy 0.9\n",
      "step 251, training accuracy 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 252, training accuracy 0.96\n",
      "step 253, training accuracy 0.98\n",
      "step 254, training accuracy 0.92\n",
      "step 255, training accuracy 0.92\n",
      "step 256, training accuracy 0.9\n",
      "step 257, training accuracy 0.94\n",
      "step 258, training accuracy 0.94\n",
      "step 259, training accuracy 0.88\n",
      "step 260, training accuracy 0.9\n",
      "step 261, training accuracy 0.92\n",
      "step 262, training accuracy 1\n",
      "step 263, training accuracy 0.88\n",
      "step 264, training accuracy 0.92\n",
      "step 265, training accuracy 0.94\n",
      "step 266, training accuracy 0.94\n",
      "step 267, training accuracy 0.86\n",
      "step 268, training accuracy 0.94\n",
      "step 269, training accuracy 0.92\n",
      "step 270, training accuracy 0.92\n",
      "step 271, training accuracy 0.92\n",
      "step 272, training accuracy 0.96\n",
      "step 273, training accuracy 0.96\n",
      "step 274, training accuracy 0.94\n",
      "step 275, training accuracy 0.94\n",
      "step 276, training accuracy 0.86\n",
      "step 277, training accuracy 0.96\n",
      "step 278, training accuracy 0.96\n",
      "step 279, training accuracy 0.88\n",
      "step 280, training accuracy 0.86\n",
      "step 281, training accuracy 0.96\n",
      "step 282, training accuracy 0.9\n",
      "step 283, training accuracy 0.92\n",
      "step 284, training accuracy 0.9\n",
      "step 285, training accuracy 0.98\n",
      "step 286, training accuracy 0.9\n",
      "step 287, training accuracy 0.92\n",
      "step 288, training accuracy 1\n",
      "step 289, training accuracy 0.9\n",
      "step 290, training accuracy 0.9\n",
      "step 291, training accuracy 0.9\n",
      "step 292, training accuracy 0.9\n",
      "step 293, training accuracy 0.88\n",
      "step 294, training accuracy 0.92\n",
      "step 295, training accuracy 0.94\n",
      "step 296, training accuracy 0.88\n",
      "step 297, training accuracy 0.92\n",
      "step 298, training accuracy 0.9\n",
      "step 299, training accuracy 0.92\n",
      "step 300, training accuracy 0.92\n",
      "test accuracy 0.9281\n",
      "step 301, training accuracy 0.84\n",
      "step 302, training accuracy 0.84\n",
      "step 303, training accuracy 0.94\n",
      "step 304, training accuracy 0.96\n",
      "step 305, training accuracy 0.88\n",
      "step 306, training accuracy 0.9\n",
      "step 307, training accuracy 0.92\n",
      "step 308, training accuracy 0.9\n",
      "step 309, training accuracy 0.94\n",
      "step 310, training accuracy 0.88\n",
      "step 311, training accuracy 0.9\n",
      "step 312, training accuracy 0.92\n",
      "step 313, training accuracy 0.96\n",
      "step 314, training accuracy 0.94\n",
      "step 315, training accuracy 0.92\n",
      "step 316, training accuracy 0.86\n",
      "step 317, training accuracy 0.98\n",
      "step 318, training accuracy 0.94\n",
      "step 319, training accuracy 0.84\n",
      "step 320, training accuracy 0.92\n",
      "step 321, training accuracy 0.92\n",
      "step 322, training accuracy 0.94\n",
      "step 323, training accuracy 0.92\n",
      "step 324, training accuracy 0.92\n",
      "step 325, training accuracy 0.98\n",
      "step 326, training accuracy 0.96\n",
      "step 327, training accuracy 0.86\n",
      "step 328, training accuracy 0.9\n",
      "step 329, training accuracy 0.94\n",
      "step 330, training accuracy 0.88\n",
      "step 331, training accuracy 0.96\n",
      "step 332, training accuracy 0.96\n",
      "step 333, training accuracy 0.98\n",
      "step 334, training accuracy 0.96\n",
      "step 335, training accuracy 0.88\n",
      "step 336, training accuracy 0.96\n",
      "step 337, training accuracy 0.92\n",
      "step 338, training accuracy 0.94\n",
      "step 339, training accuracy 0.92\n",
      "step 340, training accuracy 0.98\n",
      "step 341, training accuracy 1\n",
      "step 342, training accuracy 0.94\n",
      "step 343, training accuracy 0.92\n",
      "step 344, training accuracy 0.98\n",
      "step 345, training accuracy 0.96\n",
      "step 346, training accuracy 0.96\n",
      "step 347, training accuracy 0.88\n",
      "step 348, training accuracy 0.94\n",
      "step 349, training accuracy 0.98\n",
      "step 350, training accuracy 0.92\n",
      "step 351, training accuracy 0.9\n",
      "step 352, training accuracy 0.84\n",
      "step 353, training accuracy 0.96\n",
      "step 354, training accuracy 0.88\n",
      "step 355, training accuracy 1\n",
      "step 356, training accuracy 0.96\n",
      "step 357, training accuracy 0.98\n",
      "step 358, training accuracy 0.92\n",
      "step 359, training accuracy 0.92\n",
      "step 360, training accuracy 0.92\n",
      "step 361, training accuracy 0.98\n",
      "step 362, training accuracy 0.9\n",
      "step 363, training accuracy 0.96\n",
      "step 364, training accuracy 0.96\n",
      "step 365, training accuracy 0.98\n",
      "step 366, training accuracy 0.96\n",
      "step 367, training accuracy 0.94\n",
      "step 368, training accuracy 0.92\n",
      "step 369, training accuracy 0.98\n",
      "step 370, training accuracy 0.92\n",
      "step 371, training accuracy 0.88\n",
      "step 372, training accuracy 0.84\n",
      "step 373, training accuracy 0.9\n",
      "step 374, training accuracy 0.94\n",
      "step 375, training accuracy 0.96\n",
      "step 376, training accuracy 0.94\n",
      "step 377, training accuracy 0.9\n",
      "step 378, training accuracy 0.94\n",
      "step 379, training accuracy 0.92\n",
      "step 380, training accuracy 0.98\n",
      "step 381, training accuracy 0.94\n",
      "step 382, training accuracy 0.98\n",
      "step 383, training accuracy 0.98\n",
      "step 384, training accuracy 0.96\n",
      "step 385, training accuracy 0.96\n",
      "step 386, training accuracy 0.94\n",
      "step 387, training accuracy 0.98\n",
      "step 388, training accuracy 0.94\n",
      "step 389, training accuracy 0.98\n",
      "step 390, training accuracy 0.9\n",
      "step 391, training accuracy 0.96\n",
      "step 392, training accuracy 0.82\n",
      "step 393, training accuracy 0.9\n",
      "step 394, training accuracy 0.94\n",
      "step 395, training accuracy 0.9\n",
      "step 396, training accuracy 0.96\n",
      "step 397, training accuracy 0.94\n",
      "step 398, training accuracy 0.88\n",
      "step 399, training accuracy 0.9\n",
      "step 400, training accuracy 0.9\n",
      "test accuracy 0.934\n",
      "step 401, training accuracy 0.94\n",
      "step 402, training accuracy 0.88\n",
      "step 403, training accuracy 0.96\n",
      "step 404, training accuracy 0.94\n",
      "step 405, training accuracy 0.96\n",
      "step 406, training accuracy 1\n",
      "step 407, training accuracy 0.98\n",
      "step 408, training accuracy 0.9\n",
      "step 409, training accuracy 0.88\n",
      "step 410, training accuracy 0.92\n",
      "step 411, training accuracy 0.92\n",
      "step 412, training accuracy 0.92\n",
      "step 413, training accuracy 0.94\n",
      "step 414, training accuracy 0.88\n",
      "step 415, training accuracy 0.92\n",
      "step 416, training accuracy 0.94\n",
      "step 417, training accuracy 0.92\n",
      "step 418, training accuracy 0.94\n",
      "step 419, training accuracy 0.88\n",
      "step 420, training accuracy 0.9\n",
      "step 421, training accuracy 0.96\n",
      "step 422, training accuracy 0.88\n",
      "step 423, training accuracy 0.96\n",
      "step 424, training accuracy 0.88\n",
      "step 425, training accuracy 0.96\n",
      "step 426, training accuracy 0.92\n",
      "step 427, training accuracy 0.94\n",
      "step 428, training accuracy 0.92\n",
      "step 429, training accuracy 0.92\n",
      "step 430, training accuracy 0.88\n",
      "step 431, training accuracy 0.94\n",
      "step 432, training accuracy 0.96\n",
      "step 433, training accuracy 0.96\n",
      "step 434, training accuracy 0.82\n",
      "step 435, training accuracy 0.96\n",
      "step 436, training accuracy 0.94\n",
      "step 437, training accuracy 0.98\n",
      "step 438, training accuracy 0.98\n",
      "step 439, training accuracy 0.98\n",
      "step 440, training accuracy 0.96\n",
      "step 441, training accuracy 0.98\n",
      "step 442, training accuracy 0.96\n",
      "step 443, training accuracy 0.98\n",
      "step 444, training accuracy 0.94\n",
      "step 445, training accuracy 0.98\n",
      "step 446, training accuracy 0.86\n",
      "step 447, training accuracy 0.94\n",
      "step 448, training accuracy 0.96\n",
      "step 449, training accuracy 0.92\n",
      "step 450, training accuracy 0.94\n",
      "step 451, training accuracy 0.98\n",
      "step 452, training accuracy 0.94\n",
      "step 453, training accuracy 1\n",
      "step 454, training accuracy 0.88\n",
      "step 455, training accuracy 0.98\n",
      "step 456, training accuracy 0.98\n",
      "step 457, training accuracy 1\n",
      "step 458, training accuracy 0.92\n",
      "step 459, training accuracy 1\n",
      "step 460, training accuracy 0.9\n",
      "step 461, training accuracy 1\n",
      "step 462, training accuracy 0.92\n",
      "step 463, training accuracy 0.98\n",
      "step 464, training accuracy 0.98\n",
      "step 465, training accuracy 0.98\n",
      "step 466, training accuracy 0.9\n",
      "step 467, training accuracy 1\n",
      "step 468, training accuracy 0.92\n",
      "step 469, training accuracy 0.9\n",
      "step 470, training accuracy 0.98\n",
      "step 471, training accuracy 0.94\n",
      "step 472, training accuracy 0.94\n",
      "step 473, training accuracy 0.9\n",
      "step 474, training accuracy 0.96\n",
      "step 475, training accuracy 0.96\n",
      "step 476, training accuracy 0.98\n",
      "step 477, training accuracy 0.9\n",
      "step 478, training accuracy 0.92\n",
      "step 479, training accuracy 0.94\n",
      "step 480, training accuracy 0.94\n",
      "step 481, training accuracy 0.94\n",
      "step 482, training accuracy 0.92\n",
      "step 483, training accuracy 0.98\n",
      "step 484, training accuracy 0.96\n",
      "step 485, training accuracy 0.96\n",
      "step 486, training accuracy 0.94\n",
      "step 487, training accuracy 0.96\n",
      "step 488, training accuracy 0.96\n",
      "step 489, training accuracy 0.98\n",
      "step 490, training accuracy 1\n",
      "step 491, training accuracy 0.98\n",
      "step 492, training accuracy 1\n",
      "step 493, training accuracy 0.96\n",
      "step 494, training accuracy 0.94\n",
      "step 495, training accuracy 0.96\n",
      "step 496, training accuracy 1\n",
      "step 497, training accuracy 0.94\n",
      "step 498, training accuracy 0.94\n",
      "step 499, training accuracy 0.84\n",
      "test accuracy 0.9397\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logit)\n",
    ")\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(logit, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.image('images', reshape_images)\n",
    "tf.summary.scalar('xentropy', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "max_epoch = 5\n",
    "total_batch = int(mnist.train.num_examples/BATCH_SIZE)\n",
    "# with tf.Session() as sess:\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "test_writer = tf.summary.FileWriter('./test', sess.graph)\n",
    "for epoch in range(max_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch = mnist.train.next_batch(50, shuffle=True)\n",
    "        sess.run(train_op, \n",
    "             feed_dict={images: batch[0], labels: batch[1], keep_prob: 0.5}\n",
    "        )\n",
    "        train_acc = sess.run(accuracy,\n",
    "            feed_dict={images: batch[0], labels: batch[1], keep_prob: 1.0}\n",
    "        )\n",
    "        print('step %d, training accuracy %g' % (i, train_acc))\n",
    "        if i % 100 == 0:\n",
    "            test_acc, summary = sess.run([accuracy, merged], \n",
    "                 feed_dict={images: mnist.test.images, labels: mnist.test.labels, keep_prob: 1.0}\n",
    "            )\n",
    "            print('test accuracy %g' % test_acc)\n",
    "            test_writer.add_summary(summary, i)\n",
    "test_acc, summary = sess.run([accuracy, merged], \n",
    "     feed_dict={images: mnist.test.images, labels: mnist.test.labels, keep_prob: 1.0}\n",
    ")\n",
    "print('test accuracy %g' % test_acc)\n",
    "test_writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x0f\\n\\x08xentropy\\x15~\\x8d+>\\n\\x0f\\n\\x08accuracy\\x158\\xf8r?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard를 통해 중요한 요소들을 기록하는 코드를 추가했다.\n",
    "가장 먼저 loss가 정상적으로 줄어드는지 확인하기 위해 추가하되, loss는 스칼라값이므로 tf.summary.scalar를 통해 기록한다. 이 때 앞에 있는 parameter는 기록할 name이다.\n",
    "accuracy 또한 iteration마다 어떻게 증가하는지 확인하기 위해 기록한다.\n",
    "\n",
    "tf.summary.merge_all 함수는 tf.summary를 통해 기록하기 위해 넣은 operation을 모두 병합하여 실행하는 함수다. tf.summary.FileWriter는 기록한 내용을 파일에 쓰는 함수이다. 원하는 폴더를 argument로 넘겨주면 된다. 우리는 test set에 대한 스칼라값만 기록할 것이므로 test 폴더에 기록한다.\n",
    "\n",
    "sess.run을 통해 test set의 accuracy operation을 실행할 때, merged operation도 함께 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일에 기록한 summary를 TensorBoard로 확인하기 위해서는 해당 폴더에 접근하여,\n",
    "\n",
    "~\\test> tensorboard --logdir=.\n",
    "\n",
    "위 명령어를 입력하면 된다.\n",
    "그럼 웹브라우저에서 http://localhost:6006 를 통해 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
